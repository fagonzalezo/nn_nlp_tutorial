{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# reveal.js presentation configuration\n",
    "from notebook.services.config import ConfigManager\n",
    "\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'league',\n",
    "              'transition': 'fade',\n",
    "              'center': 'false',\n",
    "              'overview' : 'true',\n",
    "              'start_slideshow_at': 'selected'\n",
    "})\n",
    "\n",
    "# imports\n",
    "import theano\n",
    "from theano import tensor\n",
    "import codecs\n",
    "import numpy\n",
    "import sys\n",
    "from blocks import initialization\n",
    "from blocks import roles\n",
    "from blocks.model import Model\n",
    "from blocks.bricks import Linear, NDimensionalSoftmax\n",
    "from blocks.bricks.parallel import Fork\n",
    "from blocks.bricks.recurrent import GatedRecurrent\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.serialization import load_parameters\n",
    "from blocks.bricks import NDimensionalSoftmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Language modeling with RNN\n",
    "\n",
    "[Fabio A. González](http://dis.unal.edu.co/~fgonza/), Universidad Nacional de Colombia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup\n",
    "* Training data: Biblia Reina Valera 1960\n",
    "* Software:\n",
    "  * [Blocks](https://github.com/mila-udem/blocks): \"Blocks is a framework that helps you build neural network models on top of Theano\"\n",
    "  * [Theano](http://deeplearning.net/software/theano/): \"Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chars: 978848\n",
      "Vocabulary size: 85\n"
     ]
    }
   ],
   "source": [
    "# Load training file to get vocabulary\n",
    "text_file = 'biblia.txt' # input file\n",
    "with codecs.open(text_file, 'r', 'utf-8') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "print \"Total number of chars:\", len(data)\n",
    "print \"Vocabulary size:\", vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s será medido. 3 ¿Y por qué miras la paja que está en el ojo de tu hermano, y no echas de ver la viga que está en tu propio ojo? 4 ¿O cómo dirás a tu hermano: Déjame sacar la paja de tu ojo, y he aquí la viga en el ojo tuyo? 5 ¡Hipócrita! saca primero la viga de tu propio ojo, y entonces verás bien para sacar la paja del ojo de tu hermano.\r\n",
      "\r\n",
      "6 No deis lo santo a los perros, ni echéis vuestras perlas delante de los cerdos, no sea que las pisoteen, y se vuelvan y os despedacen.\r\n",
      "\r\n",
      "La oración, y la regla de oro\r\n",
      "\r\n",
      "(Lc. 11.9-13; 6.31)\r\n",
      "\r\n",
      "7 Pedid, y se os dará; buscad, y hallaréis; llamad, y se os abrirá. 8 Porque todo aquel que pide, recibe; y el que busca, halla; y al que llama, se le abrirá. 9 ¿Qué hombre hay de vosotros, que si su hijo le pide pan, le dará una piedra? 10 ¿O si le pide un pescado, le dará una serpiente? 11 Pues si vosotros, siendo malos, sabéis dar buenas dádivas a vuestros hijos, ¿cuánto más vuestro Padre que está en los cielos dará buenas cosas a los que le pidan? 12 \n"
     ]
    }
   ],
   "source": [
    "print data[21000:22000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Network architecture\n",
    "\n",
    "<img src=\"rnn_architecture.jpg\" width= 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Define the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model structure\n",
    "embedding_size = 256 # number of hidden units per layer\n",
    "\n",
    "# Input\n",
    "lookup = LookupTable(length=vocab_size, dim=embedding_size)\n",
    "\n",
    "# Layer 1\n",
    "fork1 = Fork(output_names=['linear1', 'gates1'],\n",
    "             input_dim=embedding_size, output_dims=[embedding_size, embedding_size * 2])\n",
    "fork1.name = 'fork1'\n",
    "grnn1 = GatedRecurrent(dim=embedding_size)\n",
    "grnn1.name = 'grnn1'\n",
    "\n",
    "# Layer 2\n",
    "fork2 = Fork(output_names=['linear2', 'gates2'],\n",
    "             input_dim=embedding_size, output_dims=[embedding_size, embedding_size * 2])\n",
    "fork2.name = 'fork2'\n",
    "grnn2 = GatedRecurrent(dim=embedding_size)\n",
    "grnn2.name = 'grnn2'\n",
    "\n",
    "# Softmax layer\n",
    "hidden_to_output = Linear(name='hidden_to_output', input_dim=embedding_size,\n",
    "                          output_dim=vocab_size)\n",
    "softmax = NDimensionalSoftmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Connect the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Propagate x until top brick to get y_hat predictions\n",
    "x = tensor.imatrix('features')  # input\n",
    "y = tensor.imatrix('targets')   # output\n",
    "embedding = lookup.apply(x)\n",
    "linear1, gates1 = fork1.apply(embedding)\n",
    "h1 = grnn1.apply(linear1, gates1)\n",
    "h1.name = 'h1'\n",
    "linear2, gates2 = fork2.apply(h1)\n",
    "h2 = grnn2.apply(linear2, gates2)\n",
    "h2.name = 'h2'\n",
    "linear3 = hidden_to_output.apply(h2)\n",
    "linear3.name = 'linear3'\n",
    "y_hat = softmax.apply(linear3, extra_ndim=1)\n",
    "y_hat.name = 'y_hat'\n",
    "\n",
    "# COST\n",
    "cost = softmax.categorical_cross_entropy(y, linear3, extra_ndim=1).mean()\n",
    "cost.name = 'cost'\n",
    "\n",
    "model = Model(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Load parameters and build Theano graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Load model parameters from a file\n",
    "with open('grnn_best.tar') as model_file:\n",
    "    model_params = model.get_parameter_dict().keys()\n",
    "    param_vals = {k:v for k,v in load_parameters(model_file).iteritems() if k in model_params}\n",
    "    model.set_parameter_values(param_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define Theano graph\n",
    "y, x = model.inputs\n",
    "softmax = NDimensionalSoftmax()\n",
    "linear_output = [v for v in model.variables if v.name == 'linear3'][0]\n",
    "y_hat = softmax.apply(linear_output, extra_ndim=1)\n",
    "predict = theano.function([x], y_hat)\n",
    "#theano.printing.pydotprint(predict, outfile=\"theano_graph.svg\", format = 'svg', var_with_name_simple=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"theano_graph.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#take activations of last element\n",
    "activations = [h1[-1].flatten(), h2[-1].flatten()]\n",
    "initial_states = [grnn1.parameters[-1], grnn2.parameters[-1]]\n",
    "states_as_params = [tensor.vector(dtype=initial.dtype) for initial in initial_states]\n",
    "\n",
    "#Get prob. distribution of the last element in the last seq of the batch\n",
    "fprop = theano.function([x] + states_as_params, activations + [y_hat[-1, -1, :]], givens=zip(initial_states, states_as_params))\n",
    "\n",
    "def sample(x_curr, states_values, fprop, temperature=1.0):\n",
    "    '''\n",
    "    Propagate x_curr sequence and sample next element according to\n",
    "    temperature sampling.\n",
    "    Return: sampled element and a list of the hidden activations produced by fprop.\n",
    "    '''\n",
    "    activations = fprop(x_curr, *states_values)\n",
    "    probs = activations.pop().astype('float64')\n",
    "    probs = probs / probs.sum()\n",
    "    if numpy.random.binomial(1, temperature) == 1:\n",
    "        sample = numpy.random.multinomial(1, probs).nonzero()[0][0]\n",
    "    else:\n",
    "        sample = probs.argmax()\n",
    "\n",
    "    return sample, activations, probs[sample]\n",
    "\n",
    "def init_params(primetext=u''):\n",
    "    if not primetext or len(primetext) == 0:\n",
    "        primetext = ix_to_char[numpy.random.randint(vocab_size)]\n",
    "    primetext = ''.join([ch for ch in primetext if ch in char_to_ix.keys()])\n",
    "    if len(primetext) == 0:\n",
    "        raise Exception('primetext characters are not in the vocabulary')\n",
    "    x_curr = numpy.expand_dims(\n",
    "        numpy.array([char_to_ix[ch] for ch in primetext], dtype='uint8'), axis=1)\n",
    "\n",
    "    states_values = [initial.get_value() for initial in initial_states]\n",
    "    return x_curr, states_values\n",
    "    \n",
    "def stochastic_sampling(length, primetext=u'', temperature=1.0):\n",
    "    x_curr, states_values = init_params(primetext)\n",
    "    sys.stdout.write('Starting sampling\\n' + primetext)\n",
    "    for _ in range(length):\n",
    "        idx, states_values, probs = sample(x_curr, states_values, fprop, temperature)\n",
    "        sys.stdout.write(ix_to_char[idx])\n",
    "        x_curr = [[idx]]\n",
    "\n",
    "    sys.stdout.write('\\n')\n",
    "\n",
    "def beam_sampling(length, primetext=u'', beam_size=5, temperature=1.0):\n",
    "    x_curr, states_values = init_params(primetext)\n",
    "    inputs = [x_curr] * beam_size\n",
    "    states = [states_values] * beam_size\n",
    "    logprobs = numpy.zeros((beam_size, 1))\n",
    "    seqs = numpy.zeros((steps+x_curr.shape[0], beam_size))\n",
    "    seqs[0:x_curr.shape[0], :] = numpy.repeat(x_curr, beam_size, axis=1)\n",
    "    for k in range(steps):\n",
    "        probs = numpy.zeros((beam_size,beam_size))\n",
    "        indices = numpy.zeros((beam_size,beam_size), dtype='int32')\n",
    "        hstates = numpy.empty((beam_size,beam_size), dtype=list)\n",
    "        for i in range(beam_size):\n",
    "            for j in range(beam_size):\n",
    "                indices[i][j], hstates[i][j], probs[i][j] = sample(inputs[i], states[i], fprop, temperature)\n",
    "        probs = numpy.log(probs) + logprobs\n",
    "        best_idx = probs.argmax(axis=1)\n",
    "        inputs = [[[idx]] for idx in indices[range(beam_size), best_idx]]\n",
    "        states = [hs for hs in hstates[range(beam_size), best_idx]]\n",
    "        logprobs = probs[range(beam_size), best_idx].reshape((beam_size, 1))\n",
    "        seqs[k +x_curr.shape[0], :] = numpy.array(inputs).flatten()\n",
    "\n",
    "    return logprobs.flatten(), numpy.array(seqs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log P(s) = -27.705. Sample: 5 Entonces el que había de vosotros en el pueblo, p\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "log P(s) = -27.926. Sample: 5 Y le dijo: Señor, ¿por qué había venido de la car\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "log P(s) = -29.474. Sample: 5 Y le dijo: ¿Qué había de la ciudad, para que no s\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "log P(s) = -32.863. Sample: 5 Y al ver día de reposo de la verdad, y le dijo: ¿\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "log P(s) = -33.343. Sample: 5 Porque a la verdad de la verdad está en la promes\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "logprobs, seqs = beam_sampling(50, primetext=u'')\n",
    "for i in logprobs.flatten().argsort()[::-1]:\n",
    "    print 'log P(s) = {0:3.3f}. Sample: '.format(logprobs.flatten()[i]) + u''.join([ix_to_char[ix] for ix in numpy.array(seqs).squeeze()[:,i]])\n",
    "    print '~' * 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Sampling from the model\n",
    "\n",
    "* The model calculates the probability of the next word given the previous words:  \n",
    "$$P(w_t | w_{t-1}, w_{t-2},\\dots, w_{1})$$\n",
    "* We sample from the model using this conditional probability\n",
    "  ```python\n",
    "  for i in [1..n]:\n",
    "      P = predict_next() \n",
    "      bin_var = sample_binomial(temperature)\n",
    "      if bin_var:\n",
    "          w_i = sample_multinomial(P) \n",
    "      else:\n",
    "          w_i = P.argmax() \n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sampling from the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sampling\n",
      "Y Jesús dijo: ¿Quién, pues, qué has hemos. \n",
      "\n",
      "15 Porque estaba con ellos, y le dijeron: ¿Por qué había de tu propio padre de los demonios, y le dijo: Padre que está en vosotros por la fe de la ley, que había de la pascua, y le dijo: ¿Qué harán la casa del Señor Jesucristo, el que había de los discípulos, y también la verdad de Galilea, y la palabra de vosotros te digo: Administrada esto de la ciudad. 19 Y les dijo: ¿Qué había denario de la carne, y llamado Jesús en la ley, acordaré de los incréalidades de la justicia, al que había de las palabras de la tierra, y vinieron al evangelio de la conción de la ciudad del Señor. \n",
      "\n",
      "4 Porque no seáis hablaban de la carne, y con Pablo de la cena de la multitudo a la región de fuego, y la palabra de oír la paz, Cristo en el cuerpo, diciendo: Señor, 21 que pos en el cual es de la carne, y había de la carne, y le dio a los que entrarán en la ciudad, y le dijo: ¿Qué había de predicado a los que entrésos los hombres, ya no os había de los demonios. 15 Porque también la promás, y casa de los demonios, y le vendrían este putas inmundo? 28 Entonces le fue de la circuncisión, el que había de las administras, y le respondieron que se le dijo: ¿Quién es Anados y con las mujeres habían venido de las cosas que están en vosotros, también al que hay en la carne, y le dijo: ¿Qué había de la carne, y le dijo: ¿Qué había mucho de la cabeza de ver, habiendo doctrina, la aldea, según la ley de la cual está en los cuales no le dijo: Maestro, que da hecho esto, que no se levantará al padre; y les dijo: Id a los que están en la tierra, y le había de los dejase del que les haber fruto de mí, para que habían entre los que es decimos, y atentaron, y oídos sus padres, porque no os había de la carne de la tierra. 15 Porque así también vosotros había de los enviaron; y le dijo: Yo os digo que no se ha demos a mí, y la palabra de la ciudad de la misma hora de la buena palabra del Señor Jesucristo, y la provincia de la ciudad de la multitud, y le dijo: ¿Qué quiera que hace los que están en la región del Señor Jesucristo.\n",
      "\n",
      "1 CORINTIOS.\n",
      "\n",
      "De uno de los que habían sido despedidos de muchos respecto de la carne, y levantó de los demonios, y le os recibirán; y le dijo testigos muchos de los muchos de los que anunciaban. 10 Entonces los que sabemos que de la resurrección de la ciudad de Satanás\n",
      "\n",
      "1 TIMOTEO 3\n",
      "\n",
      "1 También la mujer señal dla, y se descendencia también a los que hizo mucho de las cosas que os hablas de la carne; y le dijo: ¿Qué quiera de la casa de los demonios, y las manos de los que estaban con él estaban dur allí, y le dijo: Abraham le dijo: ¿Qué había de los profetas, y con él los que descendían de carga de la carne, y no si alguno venga al que había dejado a Dios por él, y le dijo: ¿Qué hombre es el Señor. \n",
      "\n",
      "Y le dijeron: Maestro, que está en la ciudad, y le dijo: ¿Qué había deino de los verdaderos, y le oído de la carne; porque el amor de la multitud, y le ofrendan de la fe, fue de los demandamientos del cielo, y se derramó a \n"
     ]
    }
   ],
   "source": [
    "stochastic_sampling(3000, primetext=u'Y Jesús dijo', temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probability of a text\n",
    "* The probability of a text is:  \n",
    "$$P(w_1, \\dots, w_n) = P(w_1)\\prod_{i=2}^{n}\\ P(w_i | w_{i-1},\\dots, w_{1})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate the probability of a text\n",
    "def log_likelihood(text):\n",
    "    text = ''.join([ch for ch in text if ch in char_to_ix])\n",
    "    x_curr = numpy.expand_dims(numpy.array([char_to_ix[ch] for ch in text], dtype='uint8'), axis=1)\n",
    "    probs = predict(x_curr).squeeze()\n",
    "    return sum([numpy.log(probs[i,c]) for i,c in enumerate(x_curr[1:].flatten())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Most likely phrases from a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.8389759056 hombre ama a la  \n",
      "32.0275349372   la hombre ama a\n",
      "32.3668355764   hombre ama a la\n",
      "32.3860355795 la hombre ama a  \n",
      "32.7130291137   ama a la hombre\n",
      "33.0558109689   a la hombre ama\n",
      "33.7818600005   ama la hombre a\n",
      "33.8319883609   la ama a hombre\n",
      "34.0099810799   hombre a la ama\n",
      "34.6731117631   a hombre ama la\n",
      "34.7088414131   ama a hombre la\n",
      "34.7305872297   a hombre la ama\n",
      "35.1243224444 ama a la hombre  \n",
      "35.4159899969 a la hombre ama  \n",
      "35.4609327316   la hombre a ama\n",
      "35.7856470298 la ama a hombre  \n",
      "36.0883222858   ama hombre a la\n",
      "36.1893816709   hombre ama la a\n",
      "36.7862393504   hombre la ama a\n",
      "37.1080666702   la ama hombre a\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "bow =  [u' ', 'hombre', 'ama', 'la', 'a']\n",
    "perms = [' '.join(perm) for perm in permutations(bow)]\n",
    "for p, t in sorted([(-log_likelihood(text),text) for text in perms])[:20]:\n",
    "    print p, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Least likely phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.8239402816 a hombre   la ama\n",
      "51.958106713 la a hombre   ama\n",
      "52.066835979 ama la   hombre a\n",
      "52.2481978757 la   hombre a ama\n",
      "52.4202299064 ama la   a hombre\n",
      "52.5298534369 la a   hombre ama\n",
      "52.830648843 a ama la   hombre\n",
      "52.8850183918 ama hombre la   a\n",
      "53.6537188433 hombre a ama   la\n",
      "54.0823958933 ama   la a hombre\n",
      "54.4299677624 hombre   la a ama\n",
      "54.7567527487 hombre la   a ama\n",
      "54.9093022025 ama hombre   la a\n",
      "54.9575976433 ama hombre a   la\n",
      "56.0846975897 la a ama   hombre\n",
      "56.1855978359 a ama hombre   la\n",
      "56.236034771 la   a ama hombre\n",
      "56.5873797276 hombre la a   ama\n",
      "56.740208019 la a   ama hombre\n",
      "59.1151459806 ama la a   hombre\n"
     ]
    }
   ],
   "source": [
    "perms = [' '.join(perm) for perm in permutations(bow)]\n",
    "for p, t in sorted([(-log_likelihood(text),text) for text in perms])[-20:]:\n",
    "    print p, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2908600823 mpn\n",
      "11.7298278541 mnp\n",
      "13.2396000968 nmp\n",
      "18.4572623821 npm\n",
      "22.6503256425 pmn\n",
      "------------------\n",
      "11.7298278541 mnp\n",
      "13.2396000968 nmp\n",
      "18.4572623821 npm\n",
      "22.6503256425 pmn\n",
      "23.4249039468 pnm\n"
     ]
    }
   ],
   "source": [
    "text = list(u'mnp')\n",
    "perms = [''.join(perm) for perm in permutations(text)]\n",
    "for p, t in sorted([(-log_likelihood(text),text) for text in perms])[:5]:\n",
    "    print p, t\n",
    "print \"------------------\"\n",
    "for p, t in sorted([(-log_likelihood(text),text) for text in perms])[-5:]:\n",
    "    print p, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sampling\n",
      "(Lc. 13.3--5-18)\n",
      "\n",
      "21 Endonos son las cosas que el que me envió a la mesa de la ciudad, y le después de la iglesia de buena vez del Señor Jesucristo. \n",
      "\n",
      "A quién es asurirán de las higos de la cayeron a los que están en la carne, sino que esté en Jerusaléis que está: Nos ha de los demandamientos, de los gentiles, y le dijo: ¿Quién es el que había de la carne. 3 Por esto hajo de dar apóstol de las cosa\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print stochastic_sampling(400, u\"(Lc. \", temperature = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A model trained from a different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chars: 11120595\n",
      "Vocabulary size: 152\n"
     ]
    }
   ],
   "source": [
    "# Load training file to get vocabulary\n",
    "text_file = 'reg1.txt' # input file\n",
    "with codecs.open(text_file, 'r', 'utf-8') as f:\n",
    "    data = f.read()\n",
    "chars = [u'\\u0432', u'\\u1293', u'\\u2014', u'\\u2018', u'\\u201c', u'\\U00061ba1', u' ', u'$', u'(', u',', u'\\xaf', u'\\\\', u'\\u03b1', u'0', u'\\xb3', u'4', u'\\xb7', u'8', u'\\xbb', u'\\u203c', u'\\xbf', u'@', u'\\u9839', u'\\xd7', u'\\u263b', u'\\u05e1', u'`', u'\\xe3', u'd', u'\\xe7', u'h', u'l', u'\\xef', u'p', u'\\xf3', u't', u'x', u'\\ufffd', u'|', u'\\ufeff', u'\\u2003', u'j', u'\\u200b', u'\\U000ee28c', u'\\u2013', u'\\xa0', u'#', u\"'\", u'\\xa8', u'+', u'/', u'\\xb0', u'3', u'\\xb4', u'7', u';', u'?', u'\\U00065ca1', u'\\u044b', u'\\u2022', u'[', u'_', u'\\u0161', u'\\xe0', u'c', u'\\U00061b65', u'\\xe4', u'g', u'\\xe8', u'k', u'\\u2026', u'\\xec', u'o', u's', u'\\u0434', u'\\xf4', u'w', u'{', u'\\xfc', u'\\U00061285', u'\\u2030', u'\\n', u'\\ud299', u'\\xa1', u'\"', u'&', u'\\xa9', u'*', u'\\xad', u'.', u'\\xb1', u'2', u'6', u':', u'\\xbd', u'>', u'\\u0436', u'\\u2663', u'\\u2665', u'^', u'\\xe1', u'b', u'f', u'\\xe9', u'\\u266a', u'\\xed', u'n', u'\\xf1', u'r', u'v', u'\\xf9', u'z', u'\\xfd', u'\\u266b', u'~', u'\\t', u'\\u2113', u'\\x92', u'\\u2019', u'\\u201d', u'!', u'\\u9ba0', u'\\xa2', u'%', u'\\u1ba4', u')', u'\\xaa', u'-', u'\\U0006192f', u'\\xae', u'1', u'5', u'9', u'\\xba', u'=', u'\\u0441', u'\\u0430', u'\\U000ee825', u']', u'a', u'\\xe2', u'e', u'i', u'\\xea', u'm', u'q', u'\\xf2', u'u', u'\\u92a8', u'y', u'\\xfa', u'}']\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "print \"Total number of chars:\", len(data)\n",
    "print \"Vocabulary size:\", vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "lookup = LookupTable(length=vocab_size, dim=embedding_size)\n",
    "# Layer 1\n",
    "fork1 = Fork(output_names=['linear1', 'gates1'],\n",
    "             input_dim=embedding_size, output_dims=[embedding_size, embedding_size * 2])\n",
    "fork1.name = 'fork1'\n",
    "grnn1 = GatedRecurrent(dim=embedding_size)\n",
    "grnn1.name = 'grnn1'\n",
    "# Layer 2\n",
    "fork2 = Fork(output_names=['linear2', 'gates2'],\n",
    "             input_dim=embedding_size, output_dims=[embedding_size, embedding_size * 2])\n",
    "fork2.name = 'fork2'\n",
    "grnn2 = GatedRecurrent(dim=embedding_size)\n",
    "grnn2.name = 'grnn2'\n",
    "# Softmax layer\n",
    "hidden_to_output = Linear(name='hidden_to_output', input_dim=embedding_size,\n",
    "                          output_dim=vocab_size)\n",
    "softmax = NDimensionalSoftmax()\n",
    "\n",
    "# Propagate x until top brick to get y_hat predictions\n",
    "x = tensor.imatrix('features')  # input\n",
    "y = tensor.imatrix('targets')   # output\n",
    "embedding = lookup.apply(x)\n",
    "linear1, gates1 = fork1.apply(embedding)\n",
    "h1 = grnn1.apply(linear1, gates1)\n",
    "h1.name = 'h1'\n",
    "linear2, gates2 = fork2.apply(h1)\n",
    "h2 = grnn2.apply(linear2, gates2)\n",
    "h2.name = 'h2'\n",
    "linear3 = hidden_to_output.apply(h2)\n",
    "linear3.name = 'linear3'\n",
    "y_hat = softmax.apply(linear3, extra_ndim=1)\n",
    "y_hat.name = 'y_hat'\n",
    "\n",
    "# COST\n",
    "cost = softmax.categorical_cross_entropy(y, linear3, extra_ndim=1).mean()\n",
    "cost.name = 'cost'\n",
    "\n",
    "model = Model(cost)\n",
    "\n",
    "# Load model parameters from a file\n",
    "model_file_name = 'model4.pkl'\n",
    "model_params = model.get_parameter_dict().keys()\n",
    "param_vals = {k:v for k,v in load_parameter_values(model_file_name).iteritems() if k in model_params}\n",
    "model.set_parameter_values(param_vals)\n",
    "\n",
    "# Define Theano graph\n",
    "y, x = model.inputs\n",
    "softmax = NDimensionalSoftmax()\n",
    "linear_output = [v for v in model.variables if v.name == 'linear3'][0]\n",
    "y_hat = softmax.apply(linear_output, extra_ndim=1)\n",
    "predict = theano.function([x], y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bienaventurado con la mano me la paso pensando en ti, no me hagas esperar\n",
      "y a mi compañera\n",
      "\n",
      "y siempre serás mejor que tu... receten y se mueren\n",
      "\n",
      "y solo por un beso\n",
      "y tu me miras y no te tengo\n",
      "es mi bomba pa que se acabe la mano\n",
      "y si tu quieres mami tu sabes que yo soy un tipo que no se acabe la mano\n",
      "y si tu quieres mami tu sabes que yo soy tu juguete\n",
      "y si tu te me estas tentando\n",
      "\n",
      "y si tu quieres mami tu sabes que yo soy tu nene\n",
      "\n",
      "y si tu te vas a encontrar la pared\n",
      "tu y yo en ti me mata el dembow\n",
      "es que tu me \n"
     ]
    }
   ],
   "source": [
    "print sample(500, u\"bienaventurado\", temperature = 0.1, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recuerda si tu quieres mami\n",
      "recuerda si tu quieres mami\n",
      "recuerda si tu quieres mami\n"
     ]
    }
   ],
   "source": [
    "!grep -i \"si tu quieres mami\" reg.txt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
