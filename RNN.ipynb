{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# reveal.js presentation configuration\n",
    "from notebook.services.config import ConfigManager\n",
    "\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'league',\n",
    "              'transition': 'fade',\n",
    "              'center': 'false',\n",
    "              'overview' : 'true',\n",
    "              'start_slideshow_at': 'selected'\n",
    "})\n",
    "\n",
    "# imports\n",
    "import theano\n",
    "from theano import tensor\n",
    "import codecs\n",
    "import numpy\n",
    "import sys\n",
    "from blocks import initialization\n",
    "from blocks import roles\n",
    "from blocks.model import Model\n",
    "from blocks.bricks import Linear, NDimensionalSoftmax\n",
    "from blocks.bricks.parallel import Fork\n",
    "from blocks.bricks.recurrent import GatedRecurrent\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.serialization import load_parameters\n",
    "from blocks.bricks import NDimensionalSoftmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Language modeling with RNN\n",
    "\n",
    "[Fabio A. González](http://dis.unal.edu.co/~fgonza/), Universidad Nacional de Colombia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup\n",
    "* Training data: Biblia Reina Valera 1960\n",
    "* Software:\n",
    "  * [Blocks](https://github.com/mila-udem/blocks): \"Blocks is a framework that helps you build neural network models on top of Theano\"\n",
    "  * [Theano](http://deeplearning.net/software/theano/): \"Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chars: 978848\n",
      "Vocabulary size: 85\n"
     ]
    }
   ],
   "source": [
    "# Load training file to get vocabulary\n",
    "text_file = 'biblia.txt' # input file\n",
    "with codecs.open(text_file, 'r', 'utf-8') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "print \"Total number of chars:\", len(data)\n",
    "print \"Vocabulary size:\", vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s será medido. 3 ¿Y por qué miras la paja que está en el ojo de tu hermano, y no echas de ver la viga que está en tu propio ojo? 4 ¿O cómo dirás a tu hermano: Déjame sacar la paja de tu ojo, y he aquí la viga en el ojo tuyo? 5 ¡Hipócrita! saca primero la viga de tu propio ojo, y entonces verás bien para sacar la paja del ojo de tu hermano.\r\n",
      "\r\n",
      "6 No deis lo santo a los perros, ni echéis vuestras perlas delante de los cerdos, no sea que las pisoteen, y se vuelvan y os despedacen.\r\n",
      "\r\n",
      "La oración, y la regla de oro\r\n",
      "\r\n",
      "(Lc. 11.9-13; 6.31)\r\n",
      "\r\n",
      "7 Pedid, y se os dará; buscad, y hallaréis; llamad, y se os abrirá. 8 Porque todo aquel que pide, recibe; y el que busca, halla; y al que llama, se le abrirá. 9 ¿Qué hombre hay de vosotros, que si su hijo le pide pan, le dará una piedra? 10 ¿O si le pide un pescado, le dará una serpiente? 11 Pues si vosotros, siendo malos, sabéis dar buenas dádivas a vuestros hijos, ¿cuánto más vuestro Padre que está en los cielos dará buenas cosas a los que le pidan? 12 \n"
     ]
    }
   ],
   "source": [
    "print data[21000:22000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Network architecture\n",
    "\n",
    "<img src=\"rnn_architecture.jpg\" width= 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Define the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model structure\n",
    "embedding_size = 256 # number of hidden units per layer\n",
    "\n",
    "# Input\n",
    "lookup = LookupTable(length=vocab_size, dim=embedding_size)\n",
    "\n",
    "# Layer 1\n",
    "fork1 = Fork(output_names=['linear1', 'gates1'],\n",
    "             input_dim=embedding_size, output_dims=[embedding_size, embedding_size * 2])\n",
    "fork1.name = 'fork1'\n",
    "grnn1 = GatedRecurrent(dim=embedding_size)\n",
    "grnn1.name = 'grnn1'\n",
    "\n",
    "# Layer 2\n",
    "fork2 = Fork(output_names=['linear2', 'gates2'],\n",
    "             input_dim=embedding_size, output_dims=[embedding_size, embedding_size * 2])\n",
    "fork2.name = 'fork2'\n",
    "grnn2 = GatedRecurrent(dim=embedding_size)\n",
    "grnn2.name = 'grnn2'\n",
    "\n",
    "# Softmax layer\n",
    "hidden_to_output = Linear(name='hidden_to_output', input_dim=embedding_size,\n",
    "                          output_dim=vocab_size)\n",
    "softmax = NDimensionalSoftmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Connect the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Propagate x until top brick to get y_hat predictions\n",
    "x = tensor.imatrix('features')  # input\n",
    "y = tensor.imatrix('targets')   # output\n",
    "embedding = lookup.apply(x)\n",
    "linear1, gates1 = fork1.apply(embedding)\n",
    "h1 = grnn1.apply(linear1, gates1)\n",
    "h1.name = 'h1'\n",
    "linear2, gates2 = fork2.apply(h1)\n",
    "h2 = grnn2.apply(linear2, gates2)\n",
    "h2.name = 'h2'\n",
    "linear3 = hidden_to_output.apply(h2)\n",
    "linear3.name = 'linear3'\n",
    "y_hat = softmax.apply(linear3, extra_ndim=1)\n",
    "y_hat.name = 'y_hat'\n",
    "\n",
    "# COST\n",
    "cost = softmax.categorical_cross_entropy(y, linear3, extra_ndim=1).mean()\n",
    "cost.name = 'cost'\n",
    "\n",
    "model = Model(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Load parameters and build Theano graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Load model parameters from a file\n",
    "with open('grnn_best.tar') as model_file:\n",
    "    model_params = model.get_parameter_dict().keys()\n",
    "    param_vals = {k:v for k,v in load_parameters(model_file).iteritems() if k in model_params}\n",
    "    model.set_parameter_values(param_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define Theano graph\n",
    "y, x = model.inputs\n",
    "softmax = NDimensionalSoftmax()\n",
    "linear_output = [v for v in model.variables if v.name == 'linear3'][0]\n",
    "y_hat = softmax.apply(linear_output, extra_ndim=1)\n",
    "predict = theano.function([x], y_hat)\n",
    "#theano.printing.pydotprint(predict, outfile=\"theano_graph.svg\", format = 'svg', var_with_name_simple=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"theano_graph.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#take activations of last element\n",
    "activations = [h1[-1].flatten(), h2[-1].flatten()]\n",
    "initial_states = [grnn1.parameters[-1], grnn2.parameters[-1]]\n",
    "states_as_params = [tensor.vector(dtype=initial.dtype) for initial in initial_states]\n",
    "\n",
    "#Get prob. distribution of the last element in the last seq of the batch\n",
    "fprop = theano.function([x] + states_as_params, activations + [y_hat[-1, -1, :]], givens=zip(initial_states, states_as_params))\n",
    "\n",
    "def sample(x_curr, states_values, fprop, temperature=1.0):\n",
    "    '''\n",
    "    Propagate x_curr sequence and sample next element according to\n",
    "    temperature sampling.\n",
    "    Return: sampled element and a list of the hidden activations produced by fprop.\n",
    "    '''\n",
    "    activations = fprop(x_curr, *states_values)\n",
    "    probs = activations.pop().astype('float64')\n",
    "    probs = probs / probs.sum()\n",
    "    if numpy.random.binomial(1, temperature) == 1:\n",
    "        sample = numpy.random.multinomial(1, probs).nonzero()[0][0]\n",
    "    else:\n",
    "        sample = probs.argmax()\n",
    "\n",
    "    return sample, activations, probs[sample]\n",
    "\n",
    "def init_params(ix_to_char, primetext=u''):\n",
    "    if not primetext or len(primetext) == 0:\n",
    "        primetext = ix_to_char[numpy.random.randint(vocab_size)]\n",
    "    primetext = ''.join([ch for ch in primetext if ch in char_to_ix.keys()])\n",
    "    if len(primetext) == 0:\n",
    "        raise Exception('primetext characters are not in the vocabulary')\n",
    "    x_curr = numpy.expand_dims(\n",
    "        numpy.array([char_to_ix[ch] for ch in primetext], dtype='uint8'), axis=1)\n",
    "\n",
    "    states_values = [initial.get_value() for initial in initial_states]\n",
    "    return x_curr, states_values\n",
    "    \n",
    "def stochastic_sampling(length, ix_to_char, primetext=u'', temperature=1.0):\n",
    "    x_curr, states_values = init_params(ix_to_char, primetext)\n",
    "    sys.stdout.write('Starting sampling\\n' + primetext)\n",
    "    for _ in range(length):\n",
    "        idx, states_values, probs = sample(x_curr, states_values, fprop, temperature)\n",
    "        sys.stdout.write(ix_to_char[idx])\n",
    "        x_curr = [[idx]]\n",
    "\n",
    "    sys.stdout.write('\\n')\n",
    "\n",
    "def beam_sampling(length, primetext=u'', beam_size=5, temperature=1.0):\n",
    "    x_curr, states_values = init_params(ix_to_char, primetext)\n",
    "    inputs = [x_curr] * beam_size\n",
    "    states = [states_values] * beam_size\n",
    "    logprobs = numpy.zeros((beam_size, 1))\n",
    "    seqs = numpy.zeros((length+x_curr.shape[0], beam_size))\n",
    "    seqs[0:x_curr.shape[0], :] = numpy.repeat(x_curr, beam_size, axis=1)\n",
    "    for k in range(length):\n",
    "        probs = numpy.zeros((beam_size,beam_size))\n",
    "        indices = numpy.zeros((beam_size,beam_size), dtype='int32')\n",
    "        hstates = numpy.empty((beam_size,beam_size), dtype=list)\n",
    "        for i in range(beam_size):\n",
    "            for j in range(beam_size):\n",
    "                indices[i][j], hstates[i][j], probs[i][j] = sample(inputs[i], states[i], fprop, temperature)\n",
    "        probs = numpy.log(probs) + logprobs\n",
    "        best_idx = probs.argmax(axis=1)\n",
    "        inputs = [[[idx]] for idx in indices[range(beam_size), best_idx]]\n",
    "        states = [hs for hs in hstates[range(beam_size), best_idx]]\n",
    "        logprobs = probs[range(beam_size), best_idx].reshape((beam_size, 1))\n",
    "        seqs[k +x_curr.shape[0], :] = numpy.array(inputs).flatten()\n",
    "\n",
    "    return logprobs.flatten(), numpy.array(seqs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log P(s) = -26.658. Sample: , y le habían sido después de estas cosas, para que\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "log P(s) = -27.567. Sample: , y le preguntaron, de los demonios en la palabra d\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "log P(s) = -27.903. Sample: , y le dijo: Señor, ¿por qué nos ha de los demonios\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "log P(s) = -31.325. Sample: , y le dijo: ¿Qué presentaron a la verdad, y le dij\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "log P(s) = -32.353. Sample: , y le dijo: ¿Qué harán la carne, y no se levantará\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "logprobs, seqs = beam_sampling(50, primetext=u'')\n",
    "for i in logprobs.flatten().argsort()[::-1]:\n",
    "    print 'log P(s) = {0:3.3f}. Sample: '.format(logprobs.flatten()[i]) + u''.join([ix_to_char[ix] for ix in numpy.array(seqs).squeeze()[:,i]])\n",
    "    print '~' * 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Sampling from the model\n",
    "\n",
    "* The model calculates the probability of the next word given the previous words:  \n",
    "$$P(w_t | w_{t-1}, w_{t-2},\\dots, w_{1})$$\n",
    "* We sample from the model using this conditional probability\n",
    "  ```python\n",
    "  for i in [1..n]:\n",
    "      P = predict_next() \n",
    "      bin_var = sample_binomial(temperature)\n",
    "      if bin_var:\n",
    "          w_i = sample_multinomial(P) \n",
    "      else:\n",
    "          w_i = P.argmax() \n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sampling\n",
      "Y Jesús dijo: ¿Quién es el que había de la contada de la carne, y os digo que no se había una para la palabra de Esteban las cosas o por las que estaban contigo, de la cual deseación predicaba de la común al que hablaba de la resurrección de la casa de las cosas que están en Si. 17 Entonces los que habían salido de la carne, y a la hora a nadie dice: \n",
      "\n",
      "\n",
      "Viene a la multitud, dando los del cielo, y levantarán en la carne, y le dijo: ¿Por qué no te había de la carne, pero no se lo que había sido dicho estas cosas, y le predicen con él, diciendo: ¿Quién fue llevado en tierra, de la ciudad se levantó de la ciudad de las iglesias de la tribulacione de vuestros padres, y vean. \n",
      "\n",
      "42 Entonces el que había dado a su propia otra vez a los que presentanos de los muertos, por cuanto es el que había siete esemos hay de los que con él estaban con las discípulos, y le dijeron: Maestro, Pablo. 17 Porque el que llama de la cárcel, y los demonios de los que le dijeron: ¿Quién es el que había delante de su carga, y les dijo: Señor, de bajo la paz día con que no os había de la navey, y le dijo: ¿Quién es Cristo el profeta\n",
      "\n",
      "(Mt. 15.1-13; Lc. 25.2-11)\n",
      "\n",
      "13 Entonces le dijo: ¿Qué que había la gracia, diciendo:)\n",
      "\n",
      "18 Entonces le dijo: ¿Qué había de la carne, de las cuestiones de la conciencia, y los discípulos dijeron: ¿Que al que de los que habían creído, y le dijo: Todo esto es bueno, la cual está escrito: \n",
      "\n",
      "\n",
      "16 Porque es de la carne, y le dijo: ¿Qué dio a la piedad de la justicia, no lo he después de las cosas que estaban con él lo que había encargado de la carne, y les dijo: ¿Quién es el hombre que está en la carne. 15 Porque en esto hay en que no se había de vuestro goza del cielo, y les dijo: ¿Por qué nos había de la carne, y le dijo: ¿Dónde preguntaron a la multitud, pues se después de los que están en la pazgua defendió del cielo, buscando a él en el camino; y le dijo: ¿Sabed, hijo de la cual estaba cerecido de la carne. 15 Por lo cual también vosotros, pues, que está en la carne. 12 Pues en el nombre de la vez de la multitud del hombre no puede de esto? 30 Porque el que había de la carne, y por la gente de Dios que está en los cielos y los vientos de la multitud, y le trajeron a las iglesias de la carne, y le dijo: ¿Qué había de la carne, le había de la cara de la carne, para que no se quedaron de la ver, y le dijeron: ¿Quién es el que de su manto, yo te puede el profeta Isaías\n",
      "\n",
      "13 Entonces le dijeron: ¿Quién está en la de la carne, y le dijo: ¿Qué había entregado de los demonios; y le dijeron: ¿Quién es el sepulcro, y le dijo: ¿Qué había decido de aquí, y le dijo: ¿Quién es el que había de las cosas que habían sido después de la gloria para edificar al que había de la carne, y le dijo: ¿Qué hombre alaban de la gracia de los demonios, y le dijo: ¿Qué hijo de Dios no se había de nosotros, hay dice: \n",
      "\n",
      "\n",
      "Si alguno hablaba de la carne, y le dijo: ¿Qué había de la carne, y le dijeron: ¿Quién es el camino, y le dijo: Señor, ¿por qué es el que había de la palabra d\n"
     ]
    }
   ],
   "source": [
    "stochastic_sampling(3000, ix_to_char, primetext=u'Y Jesús dijo', temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probability of a text\n",
    "* The probability of a text is:  \n",
    "$$P(w_1, \\dots, w_n) = P(w_1)\\prod_{i=2}^{n}\\ P(w_i | w_{i-1},\\dots, w_{1})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate the probability of a text\n",
    "def log_likelihood(text):\n",
    "    text = ''.join([ch for ch in text if ch in char_to_ix])\n",
    "    x_curr = numpy.expand_dims(numpy.array([char_to_ix[ch] for ch in text], dtype='uint8'), axis=1)\n",
    "    probs = predict(x_curr).squeeze()\n",
    "    return sum([numpy.log(probs[i,c]) for i,c in enumerate(x_curr[1:].flatten())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Most likely phrases from a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.8389759056 hombre ama a la  \n",
      "32.0275349372   la hombre ama a\n",
      "32.3668355764   hombre ama a la\n",
      "32.3860355795 la hombre ama a  \n",
      "32.7130291137   ama a la hombre\n",
      "33.0558109689   a la hombre ama\n",
      "33.7818600005   ama la hombre a\n",
      "33.8319883609   la ama a hombre\n",
      "34.0099810799   hombre a la ama\n",
      "34.6731117631   a hombre ama la\n",
      "34.7088414131   ama a hombre la\n",
      "34.7305872297   a hombre la ama\n",
      "35.1243224444 ama a la hombre  \n",
      "35.4159899969 a la hombre ama  \n",
      "35.4609327316   la hombre a ama\n",
      "35.7856470298 la ama a hombre  \n",
      "36.0883222858   ama hombre a la\n",
      "36.1893816709   hombre ama la a\n",
      "36.7862393504   hombre la ama a\n",
      "37.1080666702   la ama hombre a\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "bow =  [u' ', 'hombre', 'ama', 'la', 'a']\n",
    "perms = [' '.join(perm) for perm in permutations(bow)]\n",
    "for p, t in sorted([(-log_likelihood(text),text) for text in perms])[:20]:\n",
    "    print p, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Least likely phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.8239402816 a hombre   la ama\n",
      "51.958106713 la a hombre   ama\n",
      "52.066835979 ama la   hombre a\n",
      "52.2481978757 la   hombre a ama\n",
      "52.4202299064 ama la   a hombre\n",
      "52.5298534369 la a   hombre ama\n",
      "52.830648843 a ama la   hombre\n",
      "52.8850183918 ama hombre la   a\n",
      "53.6537188433 hombre a ama   la\n",
      "54.0823958933 ama   la a hombre\n",
      "54.4299677624 hombre   la a ama\n",
      "54.7567527487 hombre la   a ama\n",
      "54.9093022025 ama hombre   la a\n",
      "54.9575976433 ama hombre a   la\n",
      "56.0846975897 la a ama   hombre\n",
      "56.1855978359 a ama hombre   la\n",
      "56.236034771 la   a ama hombre\n",
      "56.5873797276 hombre la a   ama\n",
      "56.740208019 la a   ama hombre\n",
      "59.1151459806 ama la a   hombre\n"
     ]
    }
   ],
   "source": [
    "perms = [' '.join(perm) for perm in permutations(bow)]\n",
    "for p, t in sorted([(-log_likelihood(text),text) for text in perms])[-20:]:\n",
    "    print p, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2908600823 mpn\n",
      "11.7298278541 mnp\n",
      "13.2396000968 nmp\n",
      "18.4572623821 npm\n",
      "22.6503256425 pmn\n",
      "------------------\n",
      "11.7298278541 mnp\n",
      "13.2396000968 nmp\n",
      "18.4572623821 npm\n",
      "22.6503256425 pmn\n",
      "23.4249039468 pnm\n"
     ]
    }
   ],
   "source": [
    "text = list(u'mnp')\n",
    "perms = [''.join(perm) for perm in permutations(text)]\n",
    "for p, t in sorted([(-log_likelihood(text),text) for text in perms])[:5]:\n",
    "    print p, t\n",
    "print \"------------------\"\n",
    "for p, t in sorted([(-log_likelihood(text),text) for text in perms])[-5:]:\n",
    "    print p, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sampling\n",
      "(Lc. 11.1-9)\n",
      "\n",
      "13 Su madre de vosotros era como si no prometinado a Cristo por la verdad, y le dijo: ¿Qué había de destrucción, sino que me ha de en la ciudad, y en fue de la tierra, y les dijo: ¿Qué había de la hora estaba en las cuales nuestro Señor vuestra promesa de la carne, y le dijo: ¿Qué había de la tierra, \n",
      "Y le dijo: Por tanto, de manera que se letras venido de la fe de la carne, y le diste\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print stochastic_sampling(400, ix_to_char, u\"(Lc. \", temperature = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A model trained from a different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chars: 11120585\n",
      "Vocabulary size: 152\n"
     ]
    }
   ],
   "source": [
    "# Load training file to get vocabulary\n",
    "text_file = 'reg1.txt' # input file\n",
    "with codecs.open(text_file, 'r', 'utf-8') as f:\n",
    "    data = f.read()\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "print \"Total number of chars:\", len(data)\n",
    "print \"Vocabulary size:\", vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "lookup = LookupTable(length=vocab_size, dim=embedding_size)\n",
    "# Layer 1\n",
    "fork1 = Fork(output_names=['linear1', 'gates1'],\n",
    "             input_dim=embedding_size, output_dims=[embedding_size, embedding_size * 2])\n",
    "fork1.name = 'fork1'\n",
    "grnn1 = GatedRecurrent(dim=embedding_size)\n",
    "grnn1.name = 'grnn1'\n",
    "# Layer 2\n",
    "fork2 = Fork(output_names=['linear2', 'gates2'],\n",
    "             input_dim=embedding_size, output_dims=[embedding_size, embedding_size * 2])\n",
    "fork2.name = 'fork2'\n",
    "grnn2 = GatedRecurrent(dim=embedding_size)\n",
    "grnn2.name = 'grnn2'\n",
    "# Softmax layer\n",
    "hidden_to_output = Linear(name='hidden_to_output', input_dim=embedding_size,\n",
    "                          output_dim=vocab_size)\n",
    "softmax = NDimensionalSoftmax()\n",
    "\n",
    "# Propagate x until top brick to get y_hat predictions\n",
    "x = tensor.imatrix('features')  # input\n",
    "y = tensor.imatrix('targets')   # output\n",
    "embedding = lookup.apply(x)\n",
    "linear1, gates1 = fork1.apply(embedding)\n",
    "h1 = grnn1.apply(linear1, gates1)\n",
    "h1.name = 'h1'\n",
    "linear2, gates2 = fork2.apply(h1)\n",
    "h2 = grnn2.apply(linear2, gates2)\n",
    "h2.name = 'h2'\n",
    "linear3 = hidden_to_output.apply(h2)\n",
    "linear3.name = 'linear3'\n",
    "y_hat = softmax.apply(linear3, extra_ndim=1)\n",
    "y_hat.name = 'y_hat'\n",
    "\n",
    "# COST\n",
    "cost = softmax.categorical_cross_entropy(y, linear3, extra_ndim=1).mean()\n",
    "cost.name = 'cost'\n",
    "\n",
    "model = Model(cost)\n",
    "# Load model parameters from a file\n",
    "with open('reg1_best.tar') as model_file:\n",
    "    model_params = model.get_parameter_dict().keys()\n",
    "    param_vals = {k:v for k,v in load_parameters(model_file).iteritems() if k in model_params}\n",
    "    model.set_parameter_values(param_vals)\n",
    "\n",
    "#take activations of last element\n",
    "activations = [h1[-1].flatten(), h2[-1].flatten()]\n",
    "initial_states = [grnn1.parameters[-1], grnn2.parameters[-1]]\n",
    "states_as_params = [tensor.vector(dtype=initial.dtype) for initial in initial_states]\n",
    "\n",
    "#Get prob. distribution of the last element in the last seq of the batch\n",
    "fprop = theano.function([x] + states_as_params, activations + [y_hat[-1, -1, :]], givens=zip(initial_states, states_as_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sampling\n",
      "bienaventurado en la cama dalel\n",
      "no vuelvas pa meterle la bocina no la miro es agatillo\n",
      "me gustan más dando a sacarme\n",
      "no inverturas conmigo no es el movimiento\n",
      "en mi maquina reproyeda\n",
      "y la posición contigo me muero\n",
      "si en mi\n",
      "dale candela\n",
      "\n",
      "nicky jam\n",
      "y tu quiere tomar a poco\n",
      "acaricias yo te voy a dar lo que le gusto no pa la calle\n",
      "y cadera la camisa la que me pare la estación\n",
      "tranquilo y te necesite de música.\n",
      "\n",
      "pídelo que no hay como la velo\n",
      "tu cuerpo yo sigo choco\n",
      "vólverán un día\n",
      "cae mami, asi es cosa (oh!)\n",
      "ella \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print stochastic_sampling(500, ix_to_char, u'bienaventurado ', temperature = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recuerda si tu quieres mami\n",
      "recuerda si tu quieres mami\n",
      "recuerda si tu quieres mami\n"
     ]
    }
   ],
   "source": [
    "!grep -i \"si tu quieres mami\" reg1.txt \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
